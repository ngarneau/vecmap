seed: 42
iteration: 0
source_language: "en"
target_language: "de"
encoding: "utf-8"  # The character encoding for input/output
precision: "float32"  # The floating-point precision, either float16, float32 or float64
cuda: False
supervised: False  # recommended if you have a large training dictionary
semi_supervised: False  # recommended if you have a small seed dictionary
identical: False  # recommended if you have no seed dictionary but can rely on identical words
aaai2018: False  # reproduce our AAAI 2018 system
acl2017: False  # reproduce our ACL 2017 system with numeral initialization
acl2017_seed: False  # reproduce our ACL 2017 system with a seed dictionary
emnlp2016: False  # reproduce our EMNLP 2016 system
num_runs: 10
csls: 10  # use CSLS for dictionary induction


seed_dictionary_method: 'unsupervised'
# init_unsupervised: False  # use unsupervised initialization
# init_identical: False  # use identical words as the seed dictionary
# init_numerals: False  # use latin numerals (i.e. words matching [0-9]+) as the seed dictionary

batch_size: 10000 # Batch size (defaults to 10000); does not affect results, larger is usually faster but uses more memory

dim_reduction: False  # apply dimensionality reduction
orthogonal: False  # use orthogonal constrained mapping
unconstrained: False  # use unconstrained mapping
direction: "union"  # the direction for dictionary induction (defaults to union)
threshold: 0.000001  # the convergence threshold
stochastic_initial: 0.1  # initial keep probability stochastic dictionary induction (defaults to 0.1)
stochastic_multiplier: 2.0  # stochastic dictionary induction multiplier (defaults to 2.0)
stochastic_interval: 50  # stochastic dictionary induction interval (defaults to 50)

unsupervised: True # recommended if you have no seed dictionary and do not want to rely on identical words
acl2018: True # reproduce our ACL 2018 system
# init_unsupervised: True
unsupervised_vocab: 4000 # restrict the vocabulary to the top k entries for unsupervised initialization
normalize: ['unit', 'center', 'unit']
whiten: True # whiten the embeddings
reweight: 0.5
src_dewhiten: 'src'
trg_dewhiten: 'trg'
self_learning: True
vocabulary_cutoff: 20000
experiment_name: "vecmap"
embeddings_path: "./data/embeddings/{}.emb.txt"

# Evaluation parameters
retrieval: 'csls' # choices: ['nn', 'invnn', 'invsoftmax', 'csls'], help='the retrieval method (nn: standard nearest neighbor; invnn: inverted nearest neighbor; invsoftmax: inverted softmax; csls: cross-domain similarity local scaling)'
inv_temperature: 1  # the inverse temperature (only compatible with inverted softmax)'
inv_sample: None  # 'use a random subset of the source vocabulary for the inverse computations (only compatible with inverted softmax)'
dot: False  # use the dot product in the similarity computations instead of the cosine')


test: False
supercomputer: False
mlflow_output_uri: 'file:./mlruns/'
embedding_output_uri: './output'
